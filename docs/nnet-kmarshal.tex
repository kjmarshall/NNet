\documentclass[12pt,notitlepage]{article}
\usepackage{hyperref}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
% \usepackage{kbordermatrix}
% theorem preambles
\newtheorem{mydef}{Definition}
\usepackage{graphicx}			% allows us to import images
\usepackage{listings}
\lstset{
  breaklines=true
}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{cmds}
\hypersetup{colorlinks=true}
\usepackage[text={7in,10in}]{geometry}
% \setlength{\parindent}{0in}

% matrix columns
\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}
\setcounter{MaxMatrixCols}{20}

\begin{document}
\title{Neural Network Notes}
\author{Kevin J. Marshall}
\maketitle

\section{Feedforward Neural Network}
\label{sec:ff-=nn}
The basic computational units of a neural network (NN) are called neurons
and are grouped together in finite sets called layers.  The structure of a
general neural network consists of an input layer followed by a finite
set of hidden computational layers and a final output layer.  In
feedforward NNs, information flows in a single direction from the
inputs to the outputs and the computation graph may always be
interpreted as a directed acyclic
graph (DAG).  Since the computational graph is a DAG, the neural
network may always be arranged such that each layer of the feedforward
NN only depends on  the layer to its left (the input side).  This can
be accomplished by performing a topological sort on the neurons
(nodes) of the DAG and
grouping them into layers $l_{k}$ with depth $k$ where $l_{k}$ is the
set of all neurons in layer $k$.  Upon arranging the $K$ layers from
$l_{0},l_{1},\dots,l_{K}$ each layer's neuron set $l_{k}$ will only depend
on neurons from the set of layers $\{l_{j<k}\}$.

\subsection{Matrix Representation}
\label{sec:matrix-rep}

During training a set of $N$ vector valued input-output pairs $\{(\vx_{0},\vy_{0}), \dots,
(\vx_{N},\vy_{N})\}$ is fed into the network.  For each layer, $k$, the
following computation is carried out,
\begin{align}
  \label{eq:activation-component}
  o_{j}^{k} &= w_{ij}^{k}a_{i}^{k-1}\\
  \label{eq:output-component}
  a_{j}^{k} &= f( o_{j}^{k})  
\end{align}
where we have used Einstein summation notation in
Eqns.~\ref{eq:activation-component} and \ref{eq:output-component} and
where the associated vector expressions are
\begin{align}
  \label{eq:activation-matrix}
  \vect{o}^{k} &= (\vect{W}^{k})^{T}\vect{a}^{k-1}\\
  \label{eq:output-matrix}
  \vect{a}^{k} &= f(\vect{o}^{k})
\end{align}
In these equations
\begin{itemize}
\item $o_{j}^{k}$ is the output computation of neuron $j$ in layer $k$
  and $\vect{o}^{k}$ has dimensions $|n_{k}+1|$ including a bias term
\item $a_{j}^{k}$ is the nonlinear output or activation of neuron $j$ in layer $k$ after
  passing through the nonlinear function $f(\cdot)$
\item $\vect{W}^{k}$ is the weight matrix of layer $k$ such that
  elements $W_{ij}^{k}$ describe the weight associated with the $i$'th
  neuron input in layer $k-1$ to the $j$'th neuron in layer $k$.
  The dimensions of the weight matrix are $( n_{k-1} + 1 ) \times
  n_{k}$ where $n_{k-1} = |l_{k-1}|$ and $n_{k}
  = |l_{k}|$ are the number of neurons in layers $k-1$ and $k$ and
  $+1$ accounts for weights associated with the bias neuron.
\item $f(\cdot;j,k)$ is a nonlinear activation function $f: \mathbb{R}^{n_{k}} \to
  \mathbb{R}^{n_{k}}$ which may be different for each layer and each neuron.  Historically
  this function is taken to be the same for all neurons in all hidden
  layers,
  $f( \cdot; j,k ) = f( \cdot )$, and often takes the form of the
  logistic function:
  \begin{equation}
    \label{eq:logistic-fun}
    f(x) = \frac{A}{1+\exp(-r(x-x_{0}))}
  \end{equation}
  where $A$ is the curve's maximum value, $x_{0}$ is the x-value of
  the functions midpoint, and $r$ is the logistic growth rate
  (steepness of the curve).  Standard practice uses $A=1, r=1, x_{0} =
  0$ which centers the function at $x=0$ and scales the input between
  to $(0,1)$
\end{itemize}
Once the information flow reaches the output layer $l_{K}$, an error
may be computed for each input based on the nodal output values
$\hat{y}_{n}$ and the target values $y_{n}$.  Gradient decent
techniques attempt to minimize the error by adjusting the weights.
Weight update rules involve computing
\begin{equation}
  \label{eq:error-grad}
  \begin{split}
    \pderiv{E}{w^{k}_{ij}} &=
    \pderiv{E}{o_{l}^{k}}\pderiv{o_{l}^{k}}{w_{ij}^{k}}\\
    &= \pderiv{E}{o_{l}^{k}} \pderiv{}{w_{ij}^{k}}\left( w_{ml}^{k}o^{k-1}_{m} \right)\\
    &= \pderiv{E}{o_{l}^{k}}\delta_{mi}\delta_{jl}o_{m}^{k-1}\\
    &= \delta_{j}^{k}o_{i}^{k-1}
  \end{split}
\end{equation}

\subsection{Bias}
\label{sec:bias}
A bias term has been hidden in Eqn.~\ref{eq:activation-matrix} such
that $\vect{W}^{T}$ is given by
\begin{equation}
  \label{eq:wmat-explicit}
  (\vect{W}^{k})^{T}\vect{a}^{k-1} =
  \begin{bmatrix}
    w_{00} & w_{10} & \dots & w_{n_{k-1}0} & w_{b_{k-1}0} \\
    \vdots & \vdots & \vdots & \vdots & \vdots \\
    w_{0n_{k}} & w_{1n_{k}} & \dots & w_{n_{k-1}n_{k}} & w_{b_{k-1}n_{k}}
  \end{bmatrix}
  \begin{bmatrix}
    a_{0}^{k-1}\\
    \vdots\\
    a_{n_{k-1}}^{k-1}\\
    1
  \end{bmatrix}
\end{equation}
for $|l_{k}|=n_{k}$ and $|l_{k-1}|=n_{k-1}$ neurons in layers $k$ and $k-1$ and a bias
weight vector of $[w_{b_{k-1}0},\dots,w_{b_{k-1}n_{k}}]^{T}$.  In
general, layers contain a decreasing number of nodes as the depth
increases, i.e. $n_{k} \le n_{k-1}$.

\subsection{Activation Functions}
\label{sec:activation-fun}
Activation functions act on neuron layer outputs and may be interpreted as
individual layers themselves.  These functions introduce
non-linearities into the prediction model.  Several common activation
functions and their derivatives are given below.  In what follows we
assume $\vect{o}^{k}$ to be the input to the activation layer and
$\vect{a}^{k}$ to be the associated output.

\begin{itemize}
\item Identity:
  \begin{align}
    \label{eq:identity}
    a_{i}^{k} &= f(o_{i}^{k}) = o_{i}^{k}\\
    \pderiv{a_{i}^{k}}{o_{i}^{k}} &= 1
  \end{align}
\item Logistic:
  \begin{align}
    \label{eq:logistic-fun}
    a_{i}^{k} &= f(o_{i}^{k}) = \frac{1}{1+\exp(-o_{i}^{k})}\\
    \pderiv{a_{i}^{k}}{o_{i}^{k}} &=
                                    \frac{\exp(-o_{i}^{k})}{(1+\exp(-o_{i}^{k}))^{2}}
                                    = f(o_{i}^{k})(1-f(o_{i}^{k}))
  \end{align}
\item TanH:
  \begin{align}
    \label{eq:tanh-fun}
    a_{i}^{k} &= f(o_{i}^{k}) = \tanh(o_{i}^{k})\\
    \pderiv{a_{i}^{k}}{o_{i}^{k}} &= \sech^{2}(o_{i}^{k}) = 1 - f^{2}(o_{i}^{k})
  \end{align}
\item arcTan:
  \begin{align}
    \label{eq:arctan-fun}
    a_{i}^{k} &= f(o_{i}^{k}) = \arctan(o_{i}^{k})\\
    \pderiv{a_{i}^{k}}{o_{i}^{k}} &= \frac{1}{(o_{i}^{k})^{2}+1}
  \end{align}
\item ReLU (rectified linear unit)
  \begin{align}
    \label{eq:relu-fun}
    a_{i}^{k} &= f(o_{i}^{k}) =
                \begin{cases}
                  o_{i}^{k}, \qquad & o_{i}^{k} \ge 0\\
                  0, \qquad & o_{i}^{k} < 0
                \end{cases}\\
    \pderiv{a_{i}^{k}}{o_{i}^{k}} &=
                                    \begin{cases}
                                      1, \qquad & f(o_{i}^{k}) \ge 0\\
                                      0, \qquad & f(o_{i}^{k}) < 0
                                    \end{cases}
  \end{align}
\item PLU (parametric linear unit):
  \begin{align}
    \label{eq:plu-fun}
    a_{i}^{k} &= f(o_{i}^{k}) =
                \begin{cases}
                  \alpha to_{i}^{k}, \qquad & o_{i}^{k} \ge 0\\
                  0, \qquad & o_{i}^{k} < 0
                \end{cases}\\
    \pderiv{a_{i}^{k}}{o_{i}^{k}} &=
                                    \begin{cases}
                                      \alpha, \qquad & f(o_{i}^{k}) \ge 0\\
                                      0, \qquad & f(o_{i}^{k}) < 0
                                    \end{cases}
  \end{align}
\item ELU (exponential linear unit):
  \begin{align}
    \label{eq:elu-fun}
    a_{i}^{k} &= f(o_{i}^{k}) =
                \begin{cases}
                  \alpha ( \exp(o_{i}^{k}) - 1 ), \qquad & o_{i}^{k} \ge 0\\
                  0, \qquad & o_{i}^{k} < 0
                \end{cases}\\
    \pderiv{a_{i}^{k}}{o_{i}^{k}} &=
                                    \begin{cases}
                                      f(o_{i}^{k}) + \alpha, \qquad & f(o_{i}^{k}) \ge 0\\
                                      0, \qquad & f(o_{i}^{k}) < 0
                                    \end{cases}
  \end{align}
\item SoftMax:
  \begin{align}
    \label{eq:relu-fun}
    a_{i}^{k} &= f(o_{i}^{k}) = \frac{\exp(o_{i}^{k})}{\sum_{j}\exp(o_{j}^{k})}\\
    \pderiv{a_{i}^{k}}{o_{i}^{k}} &= f(o_{i}^{k})\delta_{ij} - f(o_{i}^{k})f(o_{j}^{k})
  \end{align}
\item LogSoftMax:
  \begin{align}
    \label{eq:relu-fun}
    a_{i}^{k} &= f(o_{i}^{k}) = o_{i}^{k} - \log\left(
                \sum_{j}\exp(o_{j}^{k}) \right)\\
    \pderiv{a_{i}^{k}}{o_{i}^{k}} &= f(o_{i}^{k})\delta_{ij} - f(o_{i}^{k})f(o_{j}^{k})
  \end{align}
\end{itemize}

\subsection{Loss Functions}
\label{sec:loss-fun}


\subsubsection{Mean Squared Error}
\label{sec:error-mse}
The mean squared error is a good classifier for the numeric output in
which case weight updates are computed by minimizing the error
\begin{equation}
  \label{eq:error}
  \begin{split}
    E &= \frac{1}{2N}\sum_{n}(\hat{\vy}^{n} - \vy^{n} )^{2}\\
    &= \frac{1}{2N}\sum_{n}(f(\vect{a}^{K};K)^{n} - \vy^{n} )^{2}\\
    &= \frac{1}{N}\sum_{n}E_{n}\\
  \end{split}
\end{equation}
where $\hat{\vy}^{n}$ and $\vy^{n}$ are respectively the
output (prediction) vector and target vector of the $n$'th input
$(\vx^{n},\vy^{n})$.  For the output layer we find that
Eqn.~\ref{eq:error-grad} becomes
\begin{equation}
  \label{eq:error-grad-output}
  \begin{split}
    \pderiv{E_{n}}{w^{K}_{ij}} &=
    \pderiv{E_{n}}{\hat{y}_{l}^{K}}\pderiv{\hat{y}_{l}^{K}}{o_{m}^{K}}\pderiv{\hat{o_{m}^{K}}}{w_{ij}^{K}}\\
    &= (\hat{y}_{l}^{K} - y^{p}_{l} )f'(o_{l}^{K})\delta_{ml}\delta_{in}\delta_{jm}a_{n}^{K-1}\\
    &= (\hat{y}_{j}^{K} - y^{p}_{j} )f'(o_{j}^{K})a_{i}^{K-1}
  \end{split}
\end{equation}
We note that equation \ref{eq:error-grad-output} actually describes
three layer operations which take place sequentially.  These
operations may be described from right to left in accordance with back
propagation as
\begin{enumerate}
\item Error propagation over the loss function layer,
  \begin{equation}
    \label{eq:error-loss-fcn}
    \pderiv{E^{p}}{\hat{y}_{l}^{p}} = (\hat{y}_{l}^{K} - y^{p}_{l})
  \end{equation}
\item Error propagation over the activation layer
  \begin{equation}
    \label{eq:error-act-fun}
    \pderiv{E^{p}}{a_{m}^{K}} =
    \pderiv{E^{p}}{\hat{y}_{l}^{p}}f'(o_{l}^{K})\delta_{ml} = \pderiv{E^{p}}{\hat{y}_{m}^{p}}f'(o_{m}^{K})
  \end{equation}
\item Error propagation over the last layer $K$ to find gradient updates
  \begin{equation}
    \label{eq:error-last-layer}
    \pderiv{E^{p}}{w_{ij}^{K}} =
    \pderiv{E^{p}}{o_{m}^{K}}\delta_{in}\delta_{jm}a_{n}^{K-1} = \pderiv{E^{p}}{o_{j}^{K}}a_{i}^{K-1}
  \end{equation}
\end{enumerate}
The key here is to notice that back propagation inputs are error
gradients on the forward pass inputs.  Gradient update rules for
weight matrices are obtained 
\subsection{Hidden Layers}
\label{sec:hidden-layers}

For hidden layers $l_{k}$, $0 \le k < K$, the calculation becomes,
\begin{equation}
  \label{eq:error-grad-hidden}
  \begin{split}
    \pderiv{E_{n}}{w^{k}_{ij}} &=
    \pderiv{E_{n}}{o_{l}^{k}}\pderiv{o_{l}^{k}}{w_{ij}^{k}}\\
    &=
    \pderiv{E_{n}}{o_{m}^{k+1}}\pderiv{o_{m}^{k+1}}{o_{l}^{k}}\pderiv{o_{l}^{k}}{w_{ij}^{k}}\\
    &=
    \pderiv{E_{n}}{o_{m}^{k+1}}\pderiv{o_{m}^{k+1}}{o_{l}^{k}}\delta_{jl}a_{i}^{k-1}\\
    &=
    \pderiv{E_{n}}{o_{m}^{k+1}}\pderiv{o_{m}^{k+1}}{o_{j}^{k}}a_{i}^{k-1}\\
    &=
    \pderiv{E_{n}}{o_{m}^{k+1}}\pderiv{o_{m}^{k+1}}{a_{p}^{k}}\pderiv{a_{p}^{k}}{o_{j}^{k}}o_{i}^{k-1}\\ 
    &=
    \pderiv{E_{n}}{o_{m}^{k+1}}w_{pm}^{k+1}f'(o_{p}^{k})\delta_{pj}o_{i}^{k-1}\\ 
    &= \delta^{k+1}_{m}w_{jm}^{k+1}f'(a_{j}^{k})o_{i}^{k-1}\\
  \end{split}
\end{equation}

\section{Layer Based Architecture}
\label{sec:layer-arch}
The neural network architecture described above has grouped activation
functions in with network processing layers.  Insight into the back
propagation algorithm may be obtained by separating these layer
computations.

The output layer takes $y$

\section{Things to do...}
\label{sec:things-to-do}
\begin{itemize}
\item Need some way to get data in.  If using MINST we need to
  research how to import $28 \times 28 = 756$ pixels.
\item Design API to connect layers.  If we assume layers are fully
  connected we have to initialize layers from left to right or right
  to left.  For example, $756$ inputs may funnel into hidden layer
  $l_0$ which could have $100$ neurons.  The weight matrix would then
  be $756 \times 100$ which is the same as $\#inputs \times n_{0}$.
  The second hidden layer could then have $50$ neurons leading to a
  weight matrix $\vect{W}^{1}$ of dimensions $(n_{0}+1) \times n_{1}$.
  Each weight matrix $\vect{W}^{k}$ would have dimensions $(n_{k-1}+1)
  \times n_{k}$ with an output vector, $\vect{o}^{k}$ of dimension
  $n_{k}$.
\item Need to understand learning algorithm.  Run feed forward on a
  batch set of training examples then back propagate error.

\item Test on MINST, then get threading/CUDA working...
\end{itemize}



\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
